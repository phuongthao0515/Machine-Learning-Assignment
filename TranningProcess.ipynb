{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9114994,"sourceType":"datasetVersion","datasetId":5501699},{"sourceId":90743,"sourceType":"modelInstanceVersion","modelInstanceId":76073,"modelId":100777},{"sourceId":221584,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":188998,"modelId":211004}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/wafair43/transformer-for-mt?scriptVersionId=216752989\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Attention Is All You Need","metadata":{}},{"cell_type":"markdown","source":"## Introduction\nIn This notebook, we will implement the english to german model from the paper titled, \"Attention Is All You Need\" from scratch in TensorFlow. The structure is as follows:\n- Training Data\n- Validation Data\n- Test Data\n- Transformer Model\n- Loss\n- Learning rate schedule\n- Training\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Embedding, Dropout, Softmax, Masking\nfrom tensorflow.keras.models import Model, Sequential\nimport tensorflow as tf\nimport numpy as np\nimport re\nfrom tensorflow.keras.callbacks import Callback,ModelCheckpoint,CSVLogger,History\nimport time\nimport shutil\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:44:36.179719Z","iopub.execute_input":"2025-01-09T05:44:36.180002Z","iopub.status.idle":"2025-01-09T05:44:47.740526Z","shell.execute_reply.started":"2025-01-09T05:44:36.179971Z","shell.execute_reply":"2025-01-09T05:44:47.73983Z"}},"outputs":[{"name":"stderr","text":"2025-01-09 05:44:37.992418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-09 05:44:37.992561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-09 05:44:38.127601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"tf.config.list_physical_devices('GPU')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:44:47.742078Z","iopub.execute_input":"2025-01-09T05:44:47.742557Z","iopub.status.idle":"2025-01-09T05:44:47.940441Z","shell.execute_reply.started":"2025-01-09T05:44:47.742534Z","shell.execute_reply":"2025-01-09T05:44:47.939339Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/data\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:44:47.941655Z","iopub.execute_input":"2025-01-09T05:44:47.942Z","iopub.status.idle":"2025-01-09T05:44:47.957497Z","shell.execute_reply.started":"2025-01-09T05:44:47.94197Z","shell.execute_reply":"2025-01-09T05:44:47.956774Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nsource_dir = \"/kaggle/input/transformer-en-de-nmt-wmt14-bpe/\"\ndestination_dir = \"/kaggle/working/data/transformer-en-de-nmt-wmt14-bpe/\"\n\nif not os.path.exists(destination_dir):\n    shutil.copytree(source_dir, destination_dir)\n    print(f\"Directory copied to: {destination_dir}\")\nelse:\n    print(f\"Directory already exists: {destination_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:44:47.958488Z","iopub.execute_input":"2025-01-09T05:44:47.958817Z","iopub.status.idle":"2025-01-09T05:45:03.890769Z","shell.execute_reply.started":"2025-01-09T05:44:47.958796Z","shell.execute_reply":"2025-01-09T05:45:03.889914Z"}},"outputs":[{"name":"stdout","text":"Directory copied to: /kaggle/working/data/transformer-en-de-nmt-wmt14-bpe/\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Training Data \n### Background:\n- We are implement WMT 2014 English-German NMT task.\n- In the section 5 (Training) of the paper, it mentions:\n- - ~ 4.5 million sentence pairs\n- - Sentence were encoded using byte-pair encoding.\n- - Has shared source-target vocabulary.\n- - The vocabulary size is approx 37000.\n- - Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.","metadata":{}},{"cell_type":"markdown","source":"#### Load Vocabulary","metadata":{}},{"cell_type":"code","source":"vocabulary = None\npattern = r'\\n$'\nroot = \"/kaggle/working/data/transformer-en-de-nmt-wmt14-bpe/\"\nwith open(root+\"vocab.bpe.32000\", encoding='utf8') as h:\n    vocabulary = h.readlines()\n    vocabulary = list(map(lambda word:re.sub(pattern,'', word), vocabulary))\nlen(vocabulary)\n\nspecial_tokens = ['<UNK>','<StartOfSequence>','<EndOfSequence>']\ntotal_vocab = vocabulary + special_tokens\nlen(total_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:03.892266Z","iopub.execute_input":"2025-01-09T05:45:03.892616Z","iopub.status.idle":"2025-01-09T05:45:03.939942Z","shell.execute_reply.started":"2025-01-09T05:45:03.892587Z","shell.execute_reply":"2025-01-09T05:45:03.939257Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"36711"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"#### Create a Dictionary Mapping for the vocabulary","metadata":{}},{"cell_type":"code","source":"idx   = tf.range(1,len(total_vocab)+1) # start with 1, as 0 is for padding. Has total size of total_vocab\nwords = total_vocab\n\nidx   = tf.constant(idx)\nwords = tf.constant(words)\n\nwords_idx_init = tf.lookup.KeyValueTensorInitializer(words, idx)\nwords_idx = tf.lookup.StaticHashTable(words_idx_init,-1)\n\nidx_words_init = tf.lookup.KeyValueTensorInitializer(idx, words)\nidx_words = tf.lookup.StaticHashTable(idx_words_init,'<UNK>')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:03.940911Z","iopub.execute_input":"2025-01-09T05:45:03.941177Z","iopub.status.idle":"2025-01-09T05:45:04.444689Z","shell.execute_reply.started":"2025-01-09T05:45:03.941157Z","shell.execute_reply":"2025-01-09T05:45:04.443993Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"max_seq_len = 64\nvocab_size = len(total_vocab)\nbatch_size=128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:04.447861Z","iopub.execute_input":"2025-01-09T05:45:04.448126Z","iopub.status.idle":"2025-01-09T05:45:04.45186Z","shell.execute_reply.started":"2025-01-09T05:45:04.448106Z","shell.execute_reply":"2025-01-09T05:45:04.451081Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"#### Data generator","metadata":{}},{"cell_type":"code","source":"# Training data preprocessing\ndef preprocess_data(x,y):\n   \n    \n    # Preprocess features\n    \n    e,d=x\n    \n    # split sentence\n    e= tf.strings.split(tf.strings.join([start,e,end],separator=' ')) \n    \n    # tokenize (convert to numbers)\n    e = words_idx.lookup(e) \n    \n    # Truncate sequence\n    e = e[:max_seq_len] \n    \n    #padding\n    e = tf.concat([e, tf.zeros(max_seq_len-len(e), dtype=\"int32\")],axis=0) \n    \n    \n    # split sentence\n    d = tf.strings.split(tf.strings.join([start,d,end],separator=' ')) \n    \n    # tokenize\n    d = words_idx.lookup(d) \n    \n    # Keep upto max_seq_len token\n    d = d[:max_seq_len] \n    \n    # Padding\n    d = tf.concat([d, tf.zeros(max_seq_len-len(d), dtype=\"int32\")],axis=0) \n    x = (e,d)\n\n    # Preprocess labels\n    \n    y = tf.strings.split(tf.strings.join([y,end],separator=' '))\n    y = words_idx.lookup(y)\n    y = y[:max_seq_len]\n    y = tf.one_hot(y,len(total_vocab)+1)\n    \n    #padding\n    y = tf.concat([y, tf.zeros((max_seq_len-len(y),len(total_vocab)+1))],axis=0)\n    \n    \n    \n    return (x,y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:04.452861Z","iopub.execute_input":"2025-01-09T05:45:04.45316Z","iopub.status.idle":"2025-01-09T05:45:04.464656Z","shell.execute_reply.started":"2025-01-09T05:45:04.453134Z","shell.execute_reply":"2025-01-09T05:45:04.463814Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"start = \"<StartOfSequence>\"\nend   = \"<EndOfSequence>\"\n\n\n# Dataset object for reading source english corpus (Encoder input)\ndataset_en_train = tf.data.TextLineDataset(root+\"train.tok.clean.bpe.32000.en\")\n\n\n# Dataset object for reading target german corpus (Decoder Input)\ndataset_de_train = tf.data.TextLineDataset(root+\"train.tok.clean.bpe.32000.de\")\n\n# Zipping the the features sources\ntrain_X = tf.data.Dataset.zip((dataset_en_train,dataset_de_train))\n\n# Dataset object for reading target german corpus (Label)\ndataset_label_train = tf.data.TextLineDataset(root+\"train.tok.clean.bpe.32000.de\")\n\n# Zipping features and labels\ntrain = tf.data.Dataset.zip((train_X,dataset_label_train))\n\n# Shuffle\ntrain=train.shuffle(4700000)# 4462251 sentences in the dataset\n\n# Apply Preprocessing\ntrain=train.map(preprocess_data)\n\n# Batching\ntrain=train.batch(128)\ntrain = train.prefetch(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:04.465597Z","iopub.execute_input":"2025-01-09T05:45:04.465832Z","iopub.status.idle":"2025-01-09T05:45:04.770625Z","shell.execute_reply.started":"2025-01-09T05:45:04.465813Z","shell.execute_reply":"2025-01-09T05:45:04.769922Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Validation Data\n#### Data generator","metadata":{}},{"cell_type":"code","source":"start = \"<StartOfSequence>\"\nend   = \"<EndOfSequence>\"\n\n\n# Dataset object for reading source english corpus\ndataset_en_val = tf.data.TextLineDataset([root+\"newstest2012.tok.clean.bpe.32000.en\",root+\"newstest2013.tok.clean.bpe.32000.en\" ])\ndataset_en_val = dataset_en_val.map(lambda x: tf.strings.split(tf.strings.join([start,x,end],separator=' ')))\ndataset_en_val = dataset_en_val.map(lambda x: words_idx.lookup(x))\ndataset_en_val = dataset_en_val.map(lambda x: x[:max_seq_len])\ndataset_en_val = dataset_en_val.padded_batch(batch_size,max_seq_len)\n\n# Dataset object for reading target german corpus\ndataset_de_val = tf.data.TextLineDataset([root+\"newstest2012.tok.clean.bpe.32000.de\",root+\"newstest2013.tok.clean.bpe.32000.de\" ])\ndataset_de_val = dataset_de_val.map(lambda x: tf.strings.split(tf.strings.join([start,x,end],separator=' ')))\ndataset_de_val = dataset_de_val.map(lambda x: words_idx.lookup(x))\ndataset_de_val = dataset_de_val.map(lambda x: x[:max_seq_len])\ndataset_de_val = dataset_de_val.padded_batch(batch_size,max_seq_len)\n\n\nval_X = tf.data.Dataset.zip((dataset_en_val,dataset_de_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:04.771627Z","iopub.execute_input":"2025-01-09T05:45:04.771878Z","iopub.status.idle":"2025-01-09T05:45:04.958111Z","shell.execute_reply.started":"2025-01-09T05:45:04.771858Z","shell.execute_reply":"2025-01-09T05:45:04.957437Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Dataset object for reading target german corpus\ndataset_label_val = tf.data.TextLineDataset([root+\"newstest2012.tok.clean.bpe.32000.de\",root+\"newstest2013.tok.clean.bpe.32000.de\" ])\ndataset_label_val = dataset_label_val.map(lambda x: tf.strings.split(tf.strings.join([x,end],separator=' ')))\ndataset_label_val = dataset_label_val.map(lambda x: words_idx.lookup(x))\ndataset_label_val = dataset_label_val.map(lambda x: x[:max_seq_len]) # -1 because the label doesn't have the start token. I undid this\ndataset_label_val = dataset_label_val.map(lambda x: tf.one_hot(x,len(total_vocab)+1)) #  +1 because idx ranges from idx 1 to idx len(total_vocab), but one shot always have index zero to len(total_vocab)\ndataset_label_val = dataset_label_val.padded_batch(batch_size,(max_seq_len,len(total_vocab)+1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:04.95892Z","iopub.execute_input":"2025-01-09T05:45:04.95916Z","iopub.status.idle":"2025-01-09T05:45:05.07006Z","shell.execute_reply.started":"2025-01-09T05:45:04.959141Z","shell.execute_reply":"2025-01-09T05:45:05.069152Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"val = tf.data.Dataset.zip((val_X,dataset_label_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.071113Z","iopub.execute_input":"2025-01-09T05:45:05.071374Z","iopub.status.idle":"2025-01-09T05:45:05.075844Z","shell.execute_reply.started":"2025-01-09T05:45:05.071355Z","shell.execute_reply":"2025-01-09T05:45:05.075087Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Test Data\n#### Data Generator","metadata":{}},{"cell_type":"code","source":"start = \"<StartOfSequence>\"\nend   = \"<EndOfSequence>\"\n\n\n# Dataset object for reading source english corpus\ndataset_en_test = tf.data.TextLineDataset([root+\"newstest2014.tok.clean.bpe.32000.en\",])\ndataset_en_test = dataset_en_test.map(lambda x: tf.strings.split(tf.strings.join([start,x,end],separator=' ')))\ndataset_en_test = dataset_en_test.map(lambda x: words_idx.lookup(x))\ndataset_en_test = dataset_en_test.map(lambda x: x[:max_seq_len])\ndataset_en_test = dataset_en_test.padded_batch(batch_size,max_seq_len)\n\n# Dataset object for reading target german corpus\ndataset_de_test = tf.data.TextLineDataset([root+\"newstest2014.tok.clean.bpe.32000.de\"])\ndataset_de_test = dataset_de_test.map(lambda x: tf.strings.split(tf.strings.join([start,x,end],separator=' ')))\ndataset_de_test = dataset_de_test.map(lambda x: words_idx.lookup(x))\ndataset_de_test = dataset_de_test.map(lambda x: x[:max_seq_len])\ndataset_de_test = dataset_de_test.padded_batch(batch_size,max_seq_len)\n\n\ntest_X = tf.data.Dataset.zip((dataset_en_test,dataset_de_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.07693Z","iopub.execute_input":"2025-01-09T05:45:05.077192Z","iopub.status.idle":"2025-01-09T05:45:05.21062Z","shell.execute_reply.started":"2025-01-09T05:45:05.077172Z","shell.execute_reply":"2025-01-09T05:45:05.209817Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Dataset object for reading target german corpus\ndataset_label_test = tf.data.TextLineDataset([root+\"newstest2014.tok.clean.bpe.32000.de\"])\ndataset_label_test = dataset_label_test.map(lambda x: tf.strings.split(tf.strings.join([x,end],separator=' ')))\ndataset_label_test = dataset_label_test.map(lambda x: words_idx.lookup(x))\ndataset_label_test = dataset_label_test.map(lambda x: x[:max_seq_len]) # -1 because the label doesn't have the start token. I undid this\ndataset_label_test = dataset_label_test.map(lambda x: tf.one_hot(x,len(total_vocab)+1)) #  +1 because idx ranges from idx 1 to idx len(total_vocab), but one shot always have index zero to len(total_vocab)\ndataset_label_test = dataset_label_test.padded_batch(batch_size,(max_seq_len,len(total_vocab)+1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.211891Z","iopub.execute_input":"2025-01-09T05:45:05.21229Z","iopub.status.idle":"2025-01-09T05:45:05.285732Z","shell.execute_reply.started":"2025-01-09T05:45:05.212256Z","shell.execute_reply":"2025-01-09T05:45:05.285057Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"test = tf.data.Dataset.zip((test_X,dataset_label_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.28678Z","iopub.execute_input":"2025-01-09T05:45:05.287032Z","iopub.status.idle":"2025-01-09T05:45:05.291761Z","shell.execute_reply.started":"2025-01-09T05:45:05.286997Z","shell.execute_reply":"2025-01-09T05:45:05.290927Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## The Transformer Model","metadata":{}},{"cell_type":"markdown","source":"We will be implementing the *base* model.","metadata":{}},{"cell_type":"code","source":"d_model = 512 # \nN = 6 # Number of stacks of Encoders and decoders\nh = 8 # parallel attention layers\np_drop = 0.1\ndff=2048 # first layer of the FFN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.292567Z","iopub.execute_input":"2025-01-09T05:45:05.292766Z","iopub.status.idle":"2025-01-09T05:45:05.301886Z","shell.execute_reply.started":"2025-01-09T05:45:05.292749Z","shell.execute_reply":"2025-01-09T05:45:05.301068Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class ScaledDotProductAttention(Layer):\n    def __init__(self,d_model, num_heads, **kwargs):\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.scaling_factor = tf.sqrt(d_model/num_heads)\n\n   \n        \n    def call(self, Q,K,V, causal_mask=None, mask=None): \n        \n    \n        key_mask = K._keras_mask\n        out = tf.matmul(Q,K, transpose_b=True) # matmul   \n        out = tf.divide(out,self.scaling_factor) # scaling\n\n       \n        total_mask = None\n        # Padding mask will never be none due to embedding layer always passing it  the mask\n        mask_Q_num = tf.cast(mask, \"float32\")\n        mask_K_num = tf.cast(key_mask, \"float32\")\n        mask_QK = tf.matmul(mask_Q_num[...,None],mask_K_num[:,None])\n        # Mask with illegal connections due to padding. Here, illegal connections are set True value\n        illegal_padding_mask = tf.logical_not( tf.cast(mask_QK, dtype=tf.bool) )\n        total_mask = illegal_padding_mask\n         \n    \n\n          \n        if causal_mask is not None: # this is bool mask with illegal connections set to True\n            total_mask = tf.logical_or(causal_mask[None],total_mask)\n\n        # Setting the illegal connections in the total mask to -infty to make them zero in the softmax computation\n        out += tf.cast(total_mask, tf.float32) * tf.float32.min\n\n        out = tf.nn.softmax(out,axis=-1)\n        out = out * mask_Q_num[..., None]\n        #print(out)\n        \n        \n        out = tf.matmul(out,V)\n        return out\n            \n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.302767Z","iopub.execute_input":"2025-01-09T05:45:05.302999Z","iopub.status.idle":"2025-01-09T05:45:05.312796Z","shell.execute_reply.started":"2025-01-09T05:45:05.302981Z","shell.execute_reply":"2025-01-09T05:45:05.3119Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Multi-Head Attention\n- This layer creates _h_ linear projections for Q, K, and V. Or _h_ dense layers for each (without bias term)\n- Each linear projection projects the _d\\_model_ (512 in this case) dimensional tokens in Q,K,and V to _d\\_model/h_ dimensional tokens.\n- Then it performs _h_ Scaled Dot-product Attention calculations on the _h_ tuples of lower dimensional projections of Q, K, and V and produce _h_ outputs.\n- Finally, it contenates the _h_ outputs, and performs a final linear operation.","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(Layer):\n\n    \"\"\"\n    Notes: queries, keys, and values will be projected to learned linear projections of \n    dimension d_k, d_k, d_v\n    \"\"\"\n    \n    def __init__(self,d_model, num_heads, **kwargs):\n        super(MultiHeadAttention, self).__init__( **kwargs)\n\n        \n\n        \n        self.num_heads= num_heads\n        self.scaledDotProductAttention = ScaledDotProductAttention(d_model, num_heads)\n        \n        self.W_Qs = []       # List of Query Weights matrices\n        self.W_Ks = []       # List of Key Weights matrices\n        self.W_Vs = []       # List of Values Weights matrices\n        \n        d_k = int(d_model/num_heads) # Projected key dimension\n        d_v = d_k                    # Projected value dimension\n        \n        for i in range(num_heads): \n            self.W_Qs.append(Dense(units = d_k, use_bias=False))\n            self.W_Ks.append(Dense(units = d_k, use_bias=False))\n            self.W_Vs.append(Dense(units = d_v, use_bias=False))\n\n        self.W_O =  Dense(units = d_model, use_bias=False) \n\n    def call(self, Q,K,V, causal_mask=None):\n        attentionHeads = []\n        for i in range(self.num_heads):\n\n            # Project Querys, Keys and Values\n            Q_i = self.W_Qs[i](Q) # Queries' Projection for the ith head\n            K_i = self.W_Ks[i](K) # Keys' Projection for the ith head\n            V_i = self.W_Vs[i](V) # Values' Projection for the ith head\n            \n\n            attentionHeads.append(self.scaledDotProductAttention(Q_i,K_i,V_i,causal_mask,mask=Q_i._keras_mask))\n\n        # Concatenate all the attention heads on the last (feature) axis\n        concat_heads = tf.concat(attentionHeads,axis=-1)\n\n        # Final linear layer\n        return self.W_O(concat_heads)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.31382Z","iopub.execute_input":"2025-01-09T05:45:05.314126Z","iopub.status.idle":"2025-01-09T05:45:05.327137Z","shell.execute_reply.started":"2025-01-09T05:45:05.314106Z","shell.execute_reply":"2025-01-09T05:45:05.326441Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### FFN","metadata":{}},{"cell_type":"code","source":"class FFN(Layer):\n    \n    def __init__(self,dff, **kwargs):\n        super(FFN, self).__init__()\n        self.dense1 =  Dense(units=dff, activation='relu')\n        self.dense2 =  Dense(units = d_model)\n\n    \n    def call(self, inputs):\n        x = self.dense1(inputs)\n        x = self.dense2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.328104Z","iopub.execute_input":"2025-01-09T05:45:05.328358Z","iopub.status.idle":"2025-01-09T05:45:05.341Z","shell.execute_reply.started":"2025-01-09T05:45:05.328327Z","shell.execute_reply":"2025-01-09T05:45:05.340222Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Residual/Skip connections + Layer Norm","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(Layer):\n    def __init__(self, **kwargs):\n        super(ResidualBlock, self).__init__(**kwargs)\n        self.layerNorm = LayerNormalization()\n        \n    def call(self, sublayer_in, sub_layer_out, mask=None,):\n        # masking input before addition. Remember pad values have non-zero vectors\n        if mask is not None:\n            numeric_mask = tf.cast(mask[...,None],tf.float32)\n            masked_in = sublayer_in*numeric_mask\n        else:\n            masked_in = sublayer_in\n            \n        x = tf.add(sub_layer_out, masked_in) \n        x = self.layerNorm(x)\n        return x\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.341841Z","iopub.execute_input":"2025-01-09T05:45:05.342066Z","iopub.status.idle":"2025-01-09T05:45:05.353836Z","shell.execute_reply.started":"2025-01-09T05:45:05.342043Z","shell.execute_reply":"2025-01-09T05:45:05.353208Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Sublayer 1 & 2","metadata":{}},{"cell_type":"code","source":"class SubLayer1(Layer):\n    \n    def __init__(self, d_model, num_heads, p_drop, **kwargs):\n        super(SubLayer1, self).__init__(**kwargs)\n\n        self.supports_masking = True # Because SubLayer2 needs masking     \n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.dropout = Dropout(rate=p_drop)\n        self.resid = ResidualBlock()\n\n    def call(self, Q,K,V,X, training=False, causal_mask=None, mask=None):\n        \n        x = self.mha(Q,K,V,causal_mask)\n        x = self.dropout(x,training=training)\n\n        x = self.resid(X,x,mask=mask)\n        return x\n         \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.354784Z","iopub.execute_input":"2025-01-09T05:45:05.355056Z","iopub.status.idle":"2025-01-09T05:45:05.366796Z","shell.execute_reply.started":"2025-01-09T05:45:05.355027Z","shell.execute_reply":"2025-01-09T05:45:05.365931Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class SubLayer2(Layer):\n    def __init__(self,dff,p_drop, **kwargs):\n        super(SubLayer2, self).__init__(**kwargs)\n    \n        self.ffn = FFN(dff)\n        self.dropout = Dropout(rate=p_drop)\n        self.resid = ResidualBlock()\n        \n    def call(self, X, training=False, mask=None ):\n        \n        x = self.ffn(X)\n        x = self.dropout(x, training=training)\n\n        x = self.resid(X,x,mask=mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.370977Z","iopub.execute_input":"2025-01-09T05:45:05.371349Z","iopub.status.idle":"2025-01-09T05:45:05.377001Z","shell.execute_reply.started":"2025-01-09T05:45:05.371327Z","shell.execute_reply":"2025-01-09T05:45:05.376219Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Encoder Layer\n","metadata":{}},{"cell_type":"code","source":"#### Encoder layer, not encoder\nclass EncoderLayer(Layer):\n\n    \n    def __init__(self, d_model, num_heads, dff, p_drop, **kwargs):\n        super(EncoderLayer, self).__init__()\n        self.supports_masking = True # for propogation\n                        \n        self.self_attention = SubLayer1(d_model, num_heads, p_drop)\n        self.position_wise = SubLayer2(dff, p_drop)\n\n    def call(self, encoder_input, mask=None, training=False):\n        \n        #print(\"encoder layer: \" ,mask is not None)\n        x = encoder_input\n        x = self.self_attention(x,x,x,x,training=training, mask=mask)\n        x = self.position_wise(x,training=training, mask=mask)\n        return x\n    \n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.377935Z","iopub.execute_input":"2025-01-09T05:45:05.378201Z","iopub.status.idle":"2025-01-09T05:45:05.389321Z","shell.execute_reply.started":"2025-01-09T05:45:05.378182Z","shell.execute_reply":"2025-01-09T05:45:05.388633Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Positional Embedding","metadata":{}},{"cell_type":"code","source":"def getPositionalEmbedding(seq_length=256, feature_dimension=None):\n\n\n    # Returns featrure_dimension dimensional positional encodings for each position in a sequence of length seq_length.\n    # return shape (sequ_length, feature_dimension)\n    dimensions = tf.range(feature_dimension, dtype=tf.float32)\n    positions = tf.range(seq_length, dtype=tf.float32)[...,None] # add additional dimension at the end for broadcasting\n    even_dimensions = dimensions[::2]\n    odd_dimensions   =  dimensions[1::2]\n\n    feature_dimension = tf.cast(feature_dimension, tf.float32)\n    even = tf.sin( positions/tf.pow(10000.,even_dimensions/feature_dimension)  )\n    odd  = tf.cos( positions/tf.pow(10000.,odd_dimensions/feature_dimension) )\n\n    # Since for a given position, concatenating the even and odd dimension is functinonally\n    # equivalent to interleaving the even and odd dimensions, we will just stack them as follows. (I will later try interleave as well and check the time it takes) \n\n    sequence_position_embeddings = tf.concat([even,odd], axis=-1)\n \n    return sequence_position_embeddings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.3904Z","iopub.execute_input":"2025-01-09T05:45:05.391196Z","iopub.status.idle":"2025-01-09T05:45:05.404537Z","shell.execute_reply.started":"2025-01-09T05:45:05.391165Z","shell.execute_reply":"2025-01-09T05:45:05.403694Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class PositionalEncoding(Layer):\n    \"\"\"\n    This layer takes ouput from the embedding layer. Embedding layer must have output shape of (batch, seq-len, features(d_model))\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.supports_masking=True\n\n    def call(self, inputs):\n        \n        # inputs shape should be (batch, seq-len, features(d_model))\n        shape= tf.shape(inputs)\n        seq_len, features = shape[1],shape[2]\n\n        positionalEncodings = getPositionalEmbedding(seq_len, features)\n        positionalEncodings = positionalEncodings[None,...] # add batch dimension along which to broadcast\n\n        x = tf.add(inputs,positionalEncodings)  # This is equivalent to inputs+positionalEncodings[None]. \n                                                # Here, I explicitly added a batch dimension for broadcasting, but its not needed.\n                                                # You can confirm as follows:  print(tf.math.reduce_all((inputs+positionalEncodings[None])==x))\n        return x\n\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.405377Z","iopub.execute_input":"2025-01-09T05:45:05.405631Z","iopub.status.idle":"2025-01-09T05:45:05.415791Z","shell.execute_reply.started":"2025-01-09T05:45:05.405613Z","shell.execute_reply":"2025-01-09T05:45:05.414959Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"### Decoder Layer ","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(Layer):\n    def __init__(self,d_model, num_heads,dff,p_drop, **kwargs):\n        super(DecoderLayer, self).__init__()\n        \n        self.supports_masking=True # Ensures padding mask's propogation to the next decoder layer.\n        self.masked_self_attention = SubLayer1(d_model, num_heads, p_drop) \n        self.encoder_decoder_attention = SubLayer1(d_model, num_heads, p_drop)\n        self.position_wise = SubLayer2(dff, p_drop) # Position-wise feed forward\n\n    # Important note: First are should be the tensor whose padding mask needs to be propagated because I chose the easiest way for now.\n    def call(self,  decoder_input, encoder_output, causal_mask, mask=None, training=False):\n        # tokenEmbeddings: Embeddings of tokens in a sentence\n        #self attention, so Q, K, V are all same\n        x = decoder_input\n        x = self.masked_self_attention(x, x, x, x, causal_mask=causal_mask, mask=mask, training=training)\n        x = self.encoder_decoder_attention(x,encoder_output,encoder_output,x, training=training, mask=mask)\n        x = self.position_wise(x,training=training, mask=mask)\n        return x\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.416742Z","iopub.execute_input":"2025-01-09T05:45:05.41697Z","iopub.status.idle":"2025-01-09T05:45:05.430121Z","shell.execute_reply.started":"2025-01-09T05:45:05.416952Z","shell.execute_reply":"2025-01-09T05:45:05.429312Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(Layer):\n    def __init__(self,N, d_model, num_heads,dff, p_drop, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n        self.encoder_layers = []\n        for n in range(N):\n            self.encoder_layers.append(EncoderLayer(d_model, num_heads,dff, p_drop))\n    \n    \n    def call(self, inputs, training=False):\n        x = inputs\n        for i,layer in enumerate(self.encoder_layers):\n            x = layer(x,training=training) # Each layer recieves masked tensor because the previous layer has self.supports_mask=True\n            \n        return x \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.431001Z","iopub.execute_input":"2025-01-09T05:45:05.431258Z","iopub.status.idle":"2025-01-09T05:45:05.443756Z","shell.execute_reply.started":"2025-01-09T05:45:05.43124Z","shell.execute_reply":"2025-01-09T05:45:05.443054Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(Layer):\n    \n    def __init__(self, N, d_model, num_heads, dff, seq_len, p_drop, **kwargs):\n        super(Decoder, self).__init__()\n        self.decoder_layers = []\n        self.causal_mask = tf.constant(np.triu(np.ones((seq_len,seq_len)),k=1 ), dtype=tf.bool)\n\n        for n in range(N):\n            self.decoder_layers.append(DecoderLayer(d_model, num_heads,dff, p_drop))\n\n    def call(self, enc_output,  dec_input, training=False):\n        #def call(self, encoder_output, decoder_input, mask):\n        x = dec_input\n        for layer in self.decoder_layers:\n            x= layer(x, enc_output,mask=x._keras_mask, causal_mask=self.causal_mask, training=training)\n        return x\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.444637Z","iopub.execute_input":"2025-01-09T05:45:05.444932Z","iopub.status.idle":"2025-01-09T05:45:05.455146Z","shell.execute_reply.started":"2025-01-09T05:45:05.444904Z","shell.execute_reply":"2025-01-09T05:45:05.454207Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### Embedding scaling\n","metadata":{}},{"cell_type":"code","source":"class Scaling(Layer):\n    # I created this layer, because simply doing a scalar multiplication with\n    # a tensor with _keras_mask property resulted in a tensor with no mask.\n    def __init__(self, scale, **kwargs):\n        super(Scaling,self).__init__(**kwargs)\n        self.supports_masking=True\n        self.scale=scale\n        \n    def call(self, inputs):\n        return inputs*self.scale\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.456038Z","iopub.execute_input":"2025-01-09T05:45:05.456306Z","iopub.status.idle":"2025-01-09T05:45:05.468194Z","shell.execute_reply.started":"2025-01-09T05:45:05.456287Z","shell.execute_reply":"2025-01-09T05:45:05.467355Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Transformer","metadata":{}},{"cell_type":"code","source":"class Transformer(Model):\n    def __init__(self, N, d_model, num_heads,dff,seq_len, vocab_size, p_drop, **kwargs):\n        super(Transformer,self).__init__(**kwargs)\n        self.shared_embedding = Embedding(vocab_size+1,d_model,mask_zero=True) # be becasue word indexes staart from 1, but embedding layer's embedding indexes start from 0.0 index is left for padding\n        self.positional_encoding = PositionalEncoding()\n        self.encoder = Encoder(N, d_model, num_heads,dff, p_drop)\n        self.decoder =  Decoder(N, d_model, num_heads,dff,seq_len, p_drop)\n        self.dropout1 = Dropout(p_drop)\n        self.dropout2 = Dropout(p_drop)\n        self.scale_embed =  Scaling(d_model**.5)\n\n    def call(self, input, training=False):\n        \"\"\"\n        enc_inputs : tokenized sequence of shape (batch, enc_seq_len)\n        dec_inputs : tokenized sequence of shape (batch, dec_seq_len)\n        \"\"\"\n\n        enc_inputs, dec_inputs = input\n        \n        enc_inputs = self.shared_embedding(enc_inputs)\n        enc_inputs = self.scale_embed(enc_inputs)\n        enc_inputs = self.positional_encoding(enc_inputs)  \n        enc_inputs = self.dropout1(enc_inputs, training=training)\n        \n        dec_inputs = self.shared_embedding(dec_inputs)\n        dec_inputs = self.scale_embed(dec_inputs)\n        dec_inputs = self.positional_encoding(dec_inputs)\n        dec_inputs = self.dropout2(dec_inputs, training=training)\n        \n        enc_output = self.encoder(enc_inputs, training=training)\n        decoder_output = self.decoder(enc_output , dec_inputs,training=training) # has shape(batch, dec_input, d_model)\n\n        # Final linear operation. Weight tying\n        embed_weights = self.shared_embedding.weights[0] # has shape (vocab_size,d_model)\n        embed_weights_scaled = self.scale_embed(embed_weights)\n\n        logits = tf.matmul(decoder_output,embed_weights_scaled,transpose_b=True)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.469427Z","iopub.execute_input":"2025-01-09T05:45:05.469692Z","iopub.status.idle":"2025-01-09T05:45:05.478532Z","shell.execute_reply.started":"2025-01-09T05:45:05.469673Z","shell.execute_reply":"2025-01-09T05:45:05.477894Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Loss\nThe label smoothing part.","metadata":{}},{"cell_type":"code","source":"loss_obj= tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1, reduction='none', from_logits=True)\nmask_layer = Masking()\n\ndef loss_func(y_true, y_pred):\n    \n    lab_masked = mask_layer(y_true) # create mask from labels. If a particular token in a sequence has only zeros in the feature dimension, that token is masked as padding token.\n    mask = tf.cast(lab_masked._keras_mask, tf.float32)\n    loss = loss_obj(lab_masked[:,:,1:],y_pred[:,:,1:]) # The zeroth index of logits correspond to padding, so ignore it\n    masked_loss = loss * mask # zero out the loss values corresponding to padding token vectors.\n    \n    return tf.reduce_sum(masked_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.479789Z","iopub.execute_input":"2025-01-09T05:45:05.480115Z","iopub.status.idle":"2025-01-09T05:45:05.492889Z","shell.execute_reply.started":"2025-01-09T05:45:05.480087Z","shell.execute_reply":"2025-01-09T05:45:05.492304Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"### Learning rate schedule","metadata":{}},{"cell_type":"code","source":"class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n\n    def __init__(self, warmup_steps=4000, d_model=512):\n        super(MyLRSchedule,self).__init__()\n        self.d_model = d_model#tf.cast(d_model, tf.float32)\n        self.warmup_steps=warmup_steps\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        lrate = self.d_model**-.5 * tf.minimum(step**-0.5, step*self.warmup_steps**-1.5)\n        return lrate\n\n    \n    \n    def get_config(self):\n        config = {\n            'warmup_steps':self.warmup_steps,\n            'd_model':self.d_model,\n        }\n        return config\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.493709Z","iopub.execute_input":"2025-01-09T05:45:05.493936Z","iopub.status.idle":"2025-01-09T05:45:05.507946Z","shell.execute_reply.started":"2025-01-09T05:45:05.493918Z","shell.execute_reply":"2025-01-09T05:45:05.50706Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"transformer = Transformer(N, d_model, h,dff,max_seq_len, vocab_size, p_drop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:05.508845Z","iopub.execute_input":"2025-01-09T05:45:05.509106Z","iopub.status.idle":"2025-01-09T05:45:06.561268Z","shell.execute_reply.started":"2025-01-09T05:45:05.509087Z","shell.execute_reply":"2025-01-09T05:45:06.560561Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"__Optimizer__","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(beta_1=.9,beta_2=.98,epsilon=10e-9, learning_rate=MyLRSchedule())\ntransformer.compile(optimizer=optimizer,loss=loss_func)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:06.562309Z","iopub.execute_input":"2025-01-09T05:45:06.562565Z","iopub.status.idle":"2025-01-09T05:45:06.580765Z","shell.execute_reply.started":"2025-01-09T05:45:06.562543Z","shell.execute_reply":"2025-01-09T05:45:06.579918Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## Training\n- Since kaggle has a session limit of 12 hours, model needs to be saved and reloaded to continue training.\n- We ran one epoch per session. (Run training for one epoch, then renew the session, and continue training)\n### Basic workflow\n- For each epoch, respective folders needs to be created beforehand. The root folder was exp2, and inside that folder, there were folders 'epoch1', 'epoch2', 'epoch3'.\n- Update the _epoch_ variable in the custom callback cell to ensure the files are saved in the epoch they are actually generated.\n- If first epoch:\n- - compile and fit.\n- else: \n- - load the last save model, then compile and fit\n\n__Note:__ Running the fit and validate method produces warnings regarding masks being lost. This is by design and shouldn't cause any worries. Masks aren't retained where they are not needed.\n\n__Create folders needed for the current epoch__","metadata":{}},{"cell_type":"code","source":"#!rm -r /kaggle/working/exp2/epoch0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:06.581675Z","iopub.execute_input":"2025-01-09T05:45:06.581906Z","iopub.status.idle":"2025-01-09T05:45:06.585666Z","shell.execute_reply.started":"2025-01-09T05:45:06.581887Z","shell.execute_reply":"2025-01-09T05:45:06.584782Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"\nstep_size = 7200 # 7200 batches before saving a checkpoint.\nepoch='epoch0/'# Change this for each epoch\nout_path='/kaggle/working/exp2/'+epoch\nos.makedirs(out_path, exist_ok=True)\nclass CustomCallback(Callback):\n    \n    \n    def on_batch_end(self,batch,logs):\n\n        # Write batch number as well incase we want to resume from where we left of\n        path = out_path+\"batch.txt\"\n        f = open(path, \"w\")\n        f.write(str(batch)) \n        f.write(\"\\n\")\n        f.close()\n        \n        # Save model\n        if (batch+1)%step_size==0:# +1 because zero (zeroth) batch is multiple of every number\n            #training loss\n            path = out_path+'train_loss.txt'\n            f = open(path, \"a\")\n            f.write(str(logs[\"loss\"]))\n            f.write(\"\\n\")\n            f.close()\n\n            \n            # validation loss\n            path = out_path+'val_loss.txt'\n            f = open(path, \"a\")\n            val_loss = self.model.evaluate(val)\n            f.write(str(val_loss))\n            f.write(\"\\n\")\n            f.close()\n\n            # time\n            path = out_path+'time.txt'\n            f = open(path, \"a\")\n            f.write(time.strftime(\"%x %X\"))\n            f.write(\"\\n\")\n            f.close()\n\n            # Save model\n            self.model.save_weights(root+'model_e_'+ str(round(val_loss,3))+'_.weights.h5')\n\n            \n            \n\n    # Since the dataset is not exactly a multiple of steps, we need to consider the loss values when the epoch ends\n    def on_epoch_end(self, epochs,logs):\n\n        #training loss\n            path = out_path+'train_loss.txt'\n            f = open(path, \"a\")\n            f.write(str(logs[\"loss\"]))\n            f.write(\"\\n\")\n            f.close()\n\n        \n        # validation loss\n            path = out_path+'val_loss.txt'\n            f = open(path, \"a\")\n            val_loss = self.model.evaluate(val)\n            f.write(str(val_loss))\n            f.write(\"\\n\")\n            f.close()\n\n            # time\n            path = out_path+'time.txt'\n            f = open(path, \"a\")\n            f.write(time.strftime(\"%x %X\"))\n            f.write(\"\\n\")\n            f.close()\n\n            # Save model\n            self.model.save_weights(root+'model_e_'+ str(round(val_loss,3))+'_.weights.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:06.586941Z","iopub.execute_input":"2025-01-09T05:45:06.587467Z","iopub.status.idle":"2025-01-09T05:45:06.598161Z","shell.execute_reply.started":"2025-01-09T05:45:06.587437Z","shell.execute_reply":"2025-01-09T05:45:06.597383Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"transformer = Transformer(N, d_model, h,dff,max_seq_len, vocab_size, p_drop)\noptimizer = tf.keras.optimizers.Adam(beta_1=.9,beta_2=.98,epsilon=10e-9, learning_rate=MyLRSchedule())\ntransformer.compile(optimizer=optimizer,loss=loss_func)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:06.599032Z","iopub.execute_input":"2025-01-09T05:45:06.599288Z","iopub.status.idle":"2025-01-09T05:45:07.019176Z","shell.execute_reply.started":"2025-01-09T05:45:06.599269Z","shell.execute_reply":"2025-01-09T05:45:07.018241Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"#uncomment to training\n#history = transformer.fit(train,callbacks=[CustomCallback()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:45:07.020348Z","iopub.execute_input":"2025-01-09T05:45:07.020667Z","iopub.status.idle":"2025-01-09T05:45:07.024585Z","shell.execute_reply.started":"2025-01-09T05:45:07.02064Z","shell.execute_reply":"2025-01-09T05:45:07.023747Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"#### continue training (Optional)\n- restart session, load datasets, load all classes, modify _epoch_ in the callback so the current epoch's data saved in new folder.\n- epochs 1,2,3,4,...","metadata":{}},{"cell_type":"code","source":"#transformer = Transformer(N, d_model, h,dff,max_seq_len, vocab_size, p_drop)\n#optimizer = tf.keras.optimizers.Adam(beta_1=.9,beta_2=.98,epsilon=10e-9, learning_rate=MyLRSchedule())\n#transformer.compile(optimizer=optimizer,loss=loss_func)\n\n# Uncomment the beloe code to compile and model weights and optimizer's state \n# Comment if training the first epoch (already called fit in the above cell)\n\n#transformer.fit(train.take(1))# This veryy important!!, otherwise the optimizer;s state is not loaded. The schedule depends on optimizer's state. very important!!\n#weights_path = \"/kaggle/input/transformer_attention/transformers/default/1/model_e_19466.451_.weights.h5\" # latest weights from the last epoch. Change as per your environment\n#transformer.load_weights(weights_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:49:27.975172Z","iopub.execute_input":"2025-01-09T05:49:27.975483Z","iopub.status.idle":"2025-01-09T05:49:27.981112Z","shell.execute_reply.started":"2025-01-09T05:49:27.975426Z","shell.execute_reply":"2025-01-09T05:49:27.980182Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#transformer.optimizer.iterations.numpy()# Double check if the optimizer's state is loaded correctly. This total number steps the optimizer has taken. Important for schedule","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-09T05:47:52.876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Make sure to update callback code as mentioned above before running the following cell.","metadata":{}},{"cell_type":"code","source":"# Uncomment to continue training\n#history = transformer.fit(train,callbacks=[CustomCallback()])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-09T05:47:52.877Z"}},"outputs":[],"execution_count":null}]}